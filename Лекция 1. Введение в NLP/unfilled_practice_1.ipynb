{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Практика 1","metadata":{}},{"cell_type":"markdown","source":"В этом задании необходимо будет разобраться в jupyter-ноутбуке по темам первой лекции, запустить / дописать пару строк кода и ответить на несложные вопросы. Цель ноутбука - показать базовые принципы работы с текстовыми эмбеддингами, построением задачи языкового моделирования и проблемой борьбы за уменьшение количество словаря / параметров в нейронных сетях.\n\nТакже, этот код будет полезен в качестве основы для обучения собственных персональных ассистентов в рамках дополнительных заданий. \n\nОткуда брать ноутбук: https://github.com/dmkalash/mailru_llm_course/tree/main/Лекция%201.%20Введение%20в%20NLP","metadata":{}},{"cell_type":"markdown","source":"Стоит учитывать, что в некоторых частях данного ноутбука я заведомо пожертвовал оптимальностью ради большей прозрачности кода и лучшего понимания базовых принципов работы с эмбеддингами и языковым моделированием. Как оптимизировать те или иные моменты, мы будем рассказывать в следующих лекциях и семинарах.","metadata":{}},{"cell_type":"markdown","source":"## Fasttext","metadata":{}},{"cell_type":"markdown","source":"Посмотрим, как работает векторная арифметика в подходе fasttext","metadata":{}},{"cell_type":"code","source":"import re\nimport pickle \nfrom itertools import chain\nfrom datetime import datetime\nfrom collections import defaultdict\n\nfrom typing import List, Dict, Optional, Iterable, Tuple\n\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport tokenizers\nfrom tokenizers import Tokenizer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.processors import TemplateProcessing","metadata":{"execution":{"iopub.status.busy":"2024-02-22T09:50:06.576214Z","iopub.execute_input":"2024-02-22T09:50:06.576739Z","iopub.status.idle":"2024-02-22T09:50:35.946713Z","shell.execute_reply.started":"2024-02-22T09:50:06.576693Z","shell.execute_reply":"2024-02-22T09:50:35.945952Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-22 09:50:21.360652: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-22 09:50:21.360753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-22 09:50:21.631369: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Датасет, с которым будем работать. \n\nБерем небольшое количество обучающих примеров, чтобы эксперименты можно было проводить быстро. ","metadata":{}},{"cell_type":"code","source":"# ds_name_2 = 'IlyaGusev/stihi_ru'\n\ndef get_dataset(train_size: int,\n                test_size: int,\n                ds_name_1: str = 'IlyaGusev/gazeta',\n               ): \n    \n    train_dataset = load_dataset(ds_name_1, split='train')\n    test_dataset = load_dataset(ds_name_1, split='test')\n\n    train_df = pd.DataFrame(train_dataset).iloc[:train_size]\n    print(train_df.shape)\n\n    test_df = pd.DataFrame(test_dataset)[:test_size]\n    print(test_df.shape)\n\n    train_texts = (train_df['title'] + '\\n' + train_df['text']).tolist()\n    test_texts = (test_df['title'] + '\\n' + test_df['text']).tolist()\n    \n    return train_texts, test_texts","metadata":{"execution":{"iopub.status.busy":"2024-02-21T14:50:37.190428Z","iopub.execute_input":"2024-02-21T14:50:37.191863Z","iopub.status.idle":"2024-02-21T14:50:37.205992Z","shell.execute_reply.started":"2024-02-21T14:50:37.191816Z","shell.execute_reply":"2024-02-21T14:50:37.204512Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_texts, test_texts = get_dataset(5000, 500)\nall_texts = train_texts + test_texts","metadata":{"execution":{"iopub.status.busy":"2024-02-21T14:50:37.207871Z","iopub.execute_input":"2024-02-21T14:50:37.209349Z","iopub.status.idle":"2024-02-21T14:51:18.161828Z","shell.execute_reply.started":"2024-02-21T14:50:37.209314Z","shell.execute_reply":"2024-02-21T14:51:18.160746Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304d2a26a3e54ff680ceeb7c262b268c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/2.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfcd87cbc3c44d2d91d0c24bda8a632b"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset gazeta_dataset/default (download: 636.08 MiB, generated: 632.97 MiB, post-processed: Unknown size, total: 1.24 GiB) to /root/.cache/huggingface/datasets/IlyaGusev___gazeta_dataset/default/2.0.0/e2d171980aa248bc22e0af4f8485ad69071fc8e5f3d54a253c71eb434f6694bd...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02bee05a865140948eab9f43784216d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/550M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba69ef4918584e3e94f1672aa1dd7ac6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/56.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcffc34abb574fde90a39b36a955719e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/61.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfdc06e0e0b8477fb1fe92483ce9f559"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3151fa511b7e4e6a841526177893f933"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/60964 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/6793 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/6369 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset gazeta_dataset downloaded and prepared to /root/.cache/huggingface/datasets/IlyaGusev___gazeta_dataset/default/2.0.0/e2d171980aa248bc22e0af4f8485ad69071fc8e5f3d54a253c71eb434f6694bd. Subsequent calls will reuse this data.\n(5000, 5)\n(500, 5)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fasttext","metadata":{}},{"cell_type":"markdown","source":"Посмотрим, как получать эмбеддинги текстов с помощью fasttext, и как находить похожие тексты. ","metadata":{}},{"cell_type":"markdown","source":"Эту часть задания советую выполнять локально на ноутбуке. В Kaggle-ноутбуке из-за интернета предобученный fasttext загружается слишком медленно","metadata":{}},{"cell_type":"code","source":"# !pip install fasttext==0.9.2\n\nimport fasttext.util","metadata":{},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"fasttext.util.download_model('ru', if_exists='ignore')  # Russian","metadata":{"scrolled":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":["'cc.ru.300.bin'"]},"metadata":{}}]},{"cell_type":"code","source":"ft = fasttext.load_model('cc.ru.300.bin')","metadata":{},"execution_count":6,"outputs":[{"name":"stderr","output_type":"stream","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"}]},{"cell_type":"code","source":"anchor_ind = 120\nprint(train_texts[anchor_ind][:200], '\\n')\nprint(train_texts[anchor_ind + 3][:200], '\\n')","metadata":{},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"«Чикаго» в шаге от Кубка Стэнли\n\nПосле двух поражений в Чикаго «летчикам» на своей площадке удалось отыграться, так что пятого матча все ждали с нетерпением. Его победитель заметно повышал свои шансы н \n\n\n\n«Будут бить, если что»\n\nВ ночь на понедельник резко обострилась ситуация в Кадашах, где уже третью неделю идет акция в защиту исторического наследия города. Как сообщил координатор «Московского совета» \n\n\n"}]},{"cell_type":"markdown","source":"Для нахождения похожих текстов по векторам как правило используют косинусную близость, ее можно реализовать самому, можно воспользоваться готовой реализацией","metadata":{}},{"cell_type":"code","source":"v1 = ft.get_sentence_vector(train_texts[anchor_ind].replace('\\n', ' '))\nv2 = ft.get_sentence_vector(train_texts[anchor_ind + 3].replace('\\n', ' '))\n\n# K(X, Y) = <X, Y> / (||X||*||Y||)\n\nprint( (v1 * v2).sum() / (v1 ** 2).sum() ** 0.5 / (v2 ** 2).sum() ** 0.5 )\nprint( cosine_similarity(v1.reshape(1, -1), v2.reshape(1, -1))[0][0] )","metadata":{},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"0.9070907876494414\n\n0.9070907\n"}]},{"cell_type":"markdown","source":"#### >>> Задание 1\n\nНайдите в корпусе тренировочных текстов train_texts текст, наиболее похожий по косинусному расстоянию на train_texts[anchor_ind], где anchor_ind=50. В качестве ответа выведите похожесть данных текстов (число напротив \"Похожесть:\") \n\nДля всех текстов используйте единственную нормализацию как в примере выше: .replace('\\n', ' ')","metadata":{}},{"cell_type":"code","source":"anchor_ind = 120\nneed_length = 300\n\nv1 = ft.get_sentence_vector(train_texts[anchor_ind][:need_length].replace('\\n', ' '))\n\nmax_similarity = ...\nmax_sim_text = ...\n\n\nprint(\n    'Номер текста', max_ind, '\\n',\n    'Похожесть: ', round(max_similarity, 3), '\\n',\n    'Текст: ', max_sim_text[:need_length]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В качестве ответа укажите выведенную похожесть в коде выше с той же точностью. Например: 0.395","metadata":{}},{"cell_type":"markdown","source":"Как мы видим, оба текста про спорт, пусть и содержат различную семантику. Если взять anchor_ind=12, то можно обнаружить в датасете дубликат - этого стоит избегать при обучении моделей","metadata":{"execution":{"iopub.status.busy":"2024-02-22T09:57:53.279793Z","iopub.execute_input":"2024-02-22T09:57:53.280890Z","iopub.status.idle":"2024-02-22T09:57:53.287397Z","shell.execute_reply.started":"2024-02-22T09:57:53.280855Z","shell.execute_reply":"2024-02-22T09:57:53.286293Z"}}},{"cell_type":"markdown","source":"## LM на основе n-грамм","metadata":{}},{"cell_type":"markdown","source":"### 1 этап - предобработка","metadata":{}},{"cell_type":"markdown","source":"В качестве предобработки сделаем следующее:\n* приведем все тексты к нижнему регистру\n* унифицируем все пробельные символы\n\nНапишите код для приведения всех слов в тексте к нижнему регистру в методе text_preprocess ниже","metadata":{}},{"cell_type":"markdown","source":"### 2 этап - токенизация, составление словаря","metadata":{}},{"cell_type":"markdown","source":"Выделим все слова в корпусе с помощью регекса, а также добавим туда базовые знаки пунктуации и пробел","metadata":{}},{"cell_type":"code","source":"class Tokenizer:\n    def __init__(self,\n                 token_pattern: str = '\\w+|[\\!\\?\\,\\.\\-\\:]',\n                 eos_token: str = '<EOS>',\n                 pad_token: str = '<PAD>',\n                 unk_token: str = '<UNK>'):\n        self.token_pattern = token_pattern\n        self.eos_token = eos_token\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n        \n        self.special_tokens = [self.eos_token, self.pad_token, self.unk_token]\n        self.vocab = None\n        self.inverse_vocab = None\n    \n    def text_preprocess(self, input_text: str) -> str:\n        \"\"\" Предобрабатываем один текст \"\"\"\n        input_text = ... # приведение к нижнему регистру\n        input_text = re.sub('\\s+', ' ', input_text) # унифицируем пробелы\n        input_text = input_text.strip()\n        return input_text\n    \n    def build_vocab(self, corpus: List[str]) -> None:\n        assert len(corpus)\n        all_tokens = set()\n        for text in corpus:\n            all_tokens |= set(self._tokenize(text, append_eos_token=False))\n        self.vocab = {elem: ind for ind, elem in enumerate(all_tokens)}\n        special_tokens = [self.eos_token, self.unk_token, self.pad_token]\n        for token in special_tokens:\n            self.vocab[token] = len(self.vocab) + 1\n        self.inverse_vocab = {ind: elem for elem, ind in self.vocab.items()}\n        return self\n        \n    def _tokenize(self, text: str, append_eos_token: bool = True) -> List[str]:\n        text = self.text_preprocess(text)\n        tokens = re.findall(self.token_pattern, text)\n        if append_eos_token:\n            tokens.append(self.eos_token)\n        return tokens\n    \n    def encode(self, text: str, append_eos_token: bool = True) -> List[str]:\n        \"\"\" Токенизируем текст \"\"\"\n        tokens = self._tokenize(text, append_eos_token)\n        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n        return ids\n    \n    def decode(self, input_ids: Iterable[int], remove_special_tokens: bool = False) -> str:\n        assert len(input_ids)\n        assert max(input_ids) < len(self.vocab) and min(input_ids) >= 0\n        tokens = []\n        for ind in input_ids:\n            token = self.inverse_vocab[ind]\n            if remove_special_tokens and token in self.special_tokens:\n                continue\n            tokens.append(token)\n        text = ' '.join( tokens )\n        return text","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:11.788899Z","iopub.execute_input":"2024-02-21T15:03:11.789756Z","iopub.status.idle":"2024-02-21T15:03:11.808584Z","shell.execute_reply.started":"2024-02-21T15:03:11.789725Z","shell.execute_reply":"2024-02-21T15:03:11.807519Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer().build_vocab(['вот такие прироги и ничего больше', '! ? . '])","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:14.208003Z","iopub.execute_input":"2024-02-21T15:03:14.208673Z","iopub.status.idle":"2024-02-21T15:03:14.213372Z","shell.execute_reply.started":"2024-02-21T15:03:14.208640Z","shell.execute_reply":"2024-02-21T15:03:14.212422Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tokenizer.vocab","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:15.780372Z","iopub.execute_input":"2024-02-21T15:03:15.781116Z","iopub.status.idle":"2024-02-21T15:03:15.787896Z","shell.execute_reply.started":"2024-02-21T15:03:15.781081Z","shell.execute_reply":"2024-02-21T15:03:15.787061Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'прироги': 0,\n '!': 1,\n '?': 2,\n '.': 3,\n 'и': 4,\n 'больше': 5,\n 'ничего': 6,\n 'вот': 7,\n 'такие': 8,\n '<EOS>': 10,\n '<UNK>': 11,\n '<PAD>': 12}"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.encode(text)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:17.513587Z","iopub.execute_input":"2024-02-21T15:03:17.513966Z","iopub.status.idle":"2024-02-21T15:03:17.520422Z","shell.execute_reply.started":"2024-02-21T15:03:17.513937Z","shell.execute_reply":"2024-02-21T15:03:17.519488Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[7, 8, 11, 1, 10]"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer._tokenize(text, append_eos_token=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:17.744433Z","iopub.execute_input":"2024-02-21T15:03:17.744830Z","iopub.status.idle":"2024-02-21T15:03:17.751247Z","shell.execute_reply.started":"2024-02-21T15:03:17.744801Z","shell.execute_reply":"2024-02-21T15:03:17.750269Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['вот', 'такие', 'пироги', '!']"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(tokenizer.encode(text), remove_special_tokens=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:19.192389Z","iopub.execute_input":"2024-02-21T15:03:19.193337Z","iopub.status.idle":"2024-02-21T15:03:19.199497Z","shell.execute_reply.started":"2024-02-21T15:03:19.193304Z","shell.execute_reply":"2024-02-21T15:03:19.198313Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'вот такие <UNK> ! <EOS>'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(tokenizer.encode(text), remove_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:19.449945Z","iopub.execute_input":"2024-02-21T15:03:19.450225Z","iopub.status.idle":"2024-02-21T15:03:19.456301Z","shell.execute_reply.started":"2024-02-21T15:03:19.450202Z","shell.execute_reply":"2024-02-21T15:03:19.455272Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'вот такие !'"},"metadata":{}}]},{"cell_type":"markdown","source":"### 3 этап - LM на основе n-грамм","metadata":{}},{"cell_type":"markdown","source":"Напишем класс, который будет делать языковое моделирование на основе n-грамм. Изучите код, посмотрите какие функции здесь за что отвечают.\n\nНа лекции обсуждали сглаживание Лапласа для этой модели. В методе _ get_next_token определяются статистики для рассматриваемых n-грамм. В этом коде отсутствует сглаживание Лапласа. Добавьте его, согласно формуле из презентации.\n\nК слову, если этого не сделать, то генерация будет завершаться с ошибкой - тк статистики по некоторым текстам нет, будем делить на ноль.","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\n\n\nclass StatLM:\n    def __init__(self, \n                 #vocab: Dict[str, int], \n                 tokenizer: Tokenizer,\n                 context_size: int = 2,\n                 alpha: float = 0.1,\n                 sample_top_p: Optional[float] = None\n                ):\n        \n        assert context_size >= 2\n        assert sample_top_p is None or 0.0 < sample_top_p <= 1.0\n        \n        self.context_size = context_size\n        self.tokenizer = tokenizer\n        self.alpha = alpha\n        self.sample_top_p = sample_top_p\n        \n        self.n_gramms_stat = defaultdict(int)\n        self.nx_gramms_stat = defaultdict(int)\n        \n    def get_token_by_ind(ind: int) -> str:\n        return self.tokenizer.vocab.get(ind)\n    \n    def get_ind_by_token(token: str) -> int:\n        return self.tokenizer.inverse_vocab.get(token, self.tokenizer.inverse_vocab[self.unk_token])\n        \n    #def train(self, train_token_indices: List[List[int]]):\n    def train(self, train_texts: List[str]):\n        for sentence in tqdm(train_texts, desc='train lines'):\n            sentence_ind = self.tokenizer.encode(sentence)\n            for i in range(len(sentence_ind) - self.context_size):\n                \n                seq = tuple(sentence_ind[i: i + self.context_size - 1])\n                self.n_gramms_stat[seq] += 1\n                \n                seq_x = tuple(sentence_ind[i: i + self.context_size])\n                self.nx_gramms_stat[seq_x] += 1\n                \n            seq = tuple(sentence_ind[len(sentence_ind) - self.context_size:])\n            self.n_gramms_stat[seq] += 1\n            \n    def sample_token(self, token_distribution: np.ndarray) -> int:\n        if self.sample_top_p is None:\n            return token_distribution.argmax()\n        else:\n            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))))\n            total_proba = 0.0\n            tokens_to_sample = []\n            tokens_probas = []\n            for token_proba, ind in sorted(token_distribution, reverse=True):\n                tokens_to_sample.append(ind)\n                tokens_probas.append(token_proba)\n                total_proba += token_proba\n                if total_proba >= self.sample_top_p:\n                    break\n            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n            tokens_probas = np.array(tokens_probas)\n            tokens_probas = tokens_probas / tokens_probas.sum()\n            return np.random.choice(tokens_to_sample, p=tokens_probas)\n        \n    def get_stat(self) -> Dict[str, Dict]:\n        \n        n_token_stat, nx_token_stat = {}, {}\n        for token_inds, count in self.n_gramms_stat.items():\n            n_token_stat[self.tokenizer.decode(token_inds)] = count\n        \n        for token_inds, count in self.nx_gramms_stat.items():\n            nx_token_stat[self.tokenizer.decode(token_inds)] = count\n        \n        return {\n            'n gramms stat': self.n_gramms_stat,\n            'n+1 gramms stat': self.nx_gramms_stat,\n            'n tokens stat': n_token_stat,\n            'n+1 tokens stat': nx_token_stat,\n        }\n    \n    def _get_next_token(self, tokens: List[int]) -> (int, str):\n        denominator = self.n_gramms_stat.get(tuple(tokens), 0) + ... # TODO: сглаживание Лапласа\n        numerators = []\n        for ind in self.tokenizer.inverse_vocab:\n            numerators.append(self.nx_gramms_stat.get(tuple(tokens + [ind]), 0) + ...) # TODO: сглаживание Лапласа\n        \n        token_distribution = np.array(numerators) / denominator\n        max_proba_ind = self.sample_token(token_distribution)\n        \n        next_token = self.tokenizer.inverse_vocab[max_proba_ind]\n        \n        return max_proba_ind, next_token\n            \n    def generate_token(self, text: str, remove_special_tokens: bool = False) -> Dict:\n        tokens = self.tokenizer.encode(text, append_eos_token=False)\n        tokens = tokens[-self.context_size + 1:]\n        \n        max_proba_ind, next_token = self._get_next_token(tokens)\n        \n        return {\n            'next_token': next_token,\n            'next_token_num': max_proba_ind,\n        }\n    \n    \n    def generate_text(self, text: str, max_tokens: int, remove_special_tokens: bool = False) -> Dict:\n        all_tokens = self.tokenizer.encode(text, append_eos_token=False)\n        tokens = all_tokens[-self.context_size + 1:]\n        \n        next_token = None\n        while next_token != self.tokenizer.eos_token and len(all_tokens) < max_tokens:\n            max_proba_ind, next_token = self._get_next_token(tokens)\n            all_tokens.append(max_proba_ind)\n            tokens = all_tokens[-self.context_size + 1:]\n        \n        new_text = self.tokenizer.decode(all_tokens, remove_special_tokens)\n        \n        finish_reason = 'max tokens'\n        if all_tokens[-1] == self.tokenizer.vocab[self.tokenizer.eos_token]:\n            finish_reason = 'end of text'\n        \n        return {\n            'all_tokens': all_tokens,\n            'total_text': new_text,\n            'finish_reason': finish_reason\n        }","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:21.267567Z","iopub.execute_input":"2024-02-21T15:03:21.268282Z","iopub.status.idle":"2024-02-21T15:03:21.292501Z","shell.execute_reply.started":"2024-02-21T15:03:21.268251Z","shell.execute_reply":"2024-02-21T15:03:21.291323Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Для демонстрации того, что происходит, возьмем несколько коротких цитат Джейсона Стэтхема отсюда:\n\nhttps://dzen.ru/a/ZRFaGN_gKhX6xTWW","metadata":{}},{"cell_type":"code","source":"texts = [\n    'Взял нож - режь, взял дошик - ешь.',\n    'Никогда не сдавайтесь, идите к своей цели! А если будет сложно – сдавайтесь.',\n    'Запомни: всего одна ошибка – и ты ошибся.',\n    'В жизни всегда есть две дороги: одна — первая, а другая — вторая.',\n    'Делай, как надо. Как не надо, не делай.',\n    'Работа не волк. Никто не волк. Только волк волк.',\n    'Работа не волк. Работа - ворк. А волк - это ходить.',\n    'Работа',\n    ]\n\ntrain_texts = texts[:-1]\ntest_text = texts[-1]","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:23.641563Z","iopub.execute_input":"2024-02-21T15:03:23.642385Z","iopub.status.idle":"2024-02-21T15:03:23.647640Z","shell.execute_reply.started":"2024-02-21T15:03:23.642338Z","shell.execute_reply":"2024-02-21T15:03:23.646711Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer().build_vocab(train_texts)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:25.218225Z","iopub.execute_input":"2024-02-21T15:03:25.218697Z","iopub.status.idle":"2024-02-21T15:03:25.223271Z","shell.execute_reply.started":"2024-02-21T15:03:25.218665Z","shell.execute_reply":"2024-02-21T15:03:25.222261Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"tokenizer.vocab","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-02-21T15:03:25.397981Z","iopub.execute_input":"2024-02-21T15:03:25.398291Z","iopub.status.idle":"2024-02-21T15:03:25.406154Z","shell.execute_reply.started":"2024-02-21T15:03:25.398265Z","shell.execute_reply":"2024-02-21T15:03:25.405134Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'не': 0,\n 'идите': 1,\n 'сложно': 2,\n 'есть': 3,\n ',': 4,\n '-': 5,\n 'никто': 6,\n 'работа': 7,\n 'это': 8,\n ':': 9,\n 'если': 10,\n 'запомни': 11,\n 'и': 12,\n 'ошибся': 13,\n 'ходить': 14,\n 'волк': 15,\n 'первая': 16,\n 'в': 17,\n 'будет': 18,\n 'цели': 19,\n 'жизни': 20,\n 'другая': 21,\n 'всего': 22,\n 'никогда': 23,\n '!': 24,\n 'две': 25,\n 'всегда': 26,\n 'своей': 27,\n 'ешь': 28,\n 'взял': 29,\n 'дороги': 30,\n 'сдавайтесь': 31,\n 'а': 32,\n 'ворк': 33,\n 'надо': 34,\n 'нож': 35,\n '.': 36,\n 'вторая': 37,\n 'только': 38,\n 'ты': 39,\n 'режь': 40,\n 'дошик': 41,\n 'как': 42,\n 'одна': 43,\n 'делай': 44,\n 'к': 45,\n 'ошибка': 46,\n '<EOS>': 48,\n '<UNK>': 49,\n '<PAD>': 50}"},"metadata":{}}]},{"cell_type":"code","source":"# класс, который позволяем строить и использовать языковую модель на основе n-грамм\nstat_lm = StatLM(tokenizer, context_size=2, alpha=0.1, sample_top_p = None)\n\n# \"обучаем\" модель - считаем статистики\nstat_lm.train(train_texts)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:29.936563Z","iopub.execute_input":"2024-02-21T15:03:29.936948Z","iopub.status.idle":"2024-02-21T15:03:29.957657Z","shell.execute_reply.started":"2024-02-21T15:03:29.936919Z","shell.execute_reply":"2024-02-21T15:03:29.956641Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"train lines:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c692de543c2449eb823243cbb1169034"}},"metadata":{}}]},{"cell_type":"code","source":"# можем посмотреть статистики для n-грамм\ntokens_stat = stat_lm.get_stat()\nprint(tokens_stat.keys())\ntokens_stat['n+1 tokens stat']","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-02-21T15:03:32.262566Z","iopub.execute_input":"2024-02-21T15:03:32.262952Z","iopub.status.idle":"2024-02-21T15:03:32.273912Z","shell.execute_reply.started":"2024-02-21T15:03:32.262923Z","shell.execute_reply":"2024-02-21T15:03:32.272879Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"dict_keys(['n gramms stat', 'n+1 gramms stat', 'n tokens stat', 'n+1 tokens stat'])\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'взял нож': 1,\n 'нож -': 1,\n '- режь': 1,\n 'режь ,': 1,\n ', взял': 1,\n 'взял дошик': 1,\n 'дошик -': 1,\n '- ешь': 1,\n 'ешь .': 1,\n 'никогда не': 1,\n 'не сдавайтесь': 1,\n 'сдавайтесь ,': 1,\n ', идите': 1,\n 'идите к': 1,\n 'к своей': 1,\n 'своей цели': 1,\n 'цели !': 1,\n '! а': 1,\n 'а если': 1,\n 'если будет': 1,\n 'будет сложно': 1,\n 'сложно сдавайтесь': 1,\n 'сдавайтесь .': 1,\n 'запомни :': 1,\n ': всего': 1,\n 'всего одна': 1,\n 'одна ошибка': 1,\n 'ошибка и': 1,\n 'и ты': 1,\n 'ты ошибся': 1,\n 'ошибся .': 1,\n 'в жизни': 1,\n 'жизни всегда': 1,\n 'всегда есть': 1,\n 'есть две': 1,\n 'две дороги': 1,\n 'дороги :': 1,\n ': одна': 1,\n 'одна первая': 1,\n 'первая ,': 1,\n ', а': 1,\n 'а другая': 1,\n 'другая вторая': 1,\n 'вторая .': 1,\n 'делай ,': 1,\n ', как': 1,\n 'как надо': 1,\n 'надо .': 1,\n '. как': 1,\n 'как не': 1,\n 'не надо': 1,\n 'надо ,': 1,\n ', не': 1,\n 'не делай': 1,\n 'делай .': 1,\n 'работа не': 2,\n 'не волк': 3,\n 'волк .': 4,\n '. никто': 1,\n 'никто не': 1,\n '. только': 1,\n 'только волк': 1,\n 'волк волк': 1,\n '. работа': 1,\n 'работа -': 1,\n '- ворк': 1,\n 'ворк .': 1,\n '. а': 1,\n 'а волк': 1,\n 'волк -': 1,\n '- это': 1,\n 'это ходить': 1,\n 'ходить .': 1}"},"metadata":{}}]},{"cell_type":"code","source":"print('Вход:', test_text)\nprint('Новый токен:', stat_lm.generate_token(test_text)['next_token'])","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:34.658010Z","iopub.execute_input":"2024-02-21T15:03:34.658766Z","iopub.status.idle":"2024-02-21T15:03:34.666999Z","shell.execute_reply.started":"2024-02-21T15:03:34.658733Z","shell.execute_reply":"2024-02-21T15:03:34.665955Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Вход: Работа\nНовый токен: не\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'Вход: \"{test_text}\"')\ngenerated = stat_lm.generate_text(test_text, max_tokens=32)\nprint(f\"Продолженный текст: \\\"{generated['total_text']}\\\"\")\nprint(f\"Причина остановки генерации: \\\"{generated['finish_reason']}\\\"\" )","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:37.803391Z","iopub.execute_input":"2024-02-21T15:03:37.803765Z","iopub.status.idle":"2024-02-21T15:03:37.811690Z","shell.execute_reply.started":"2024-02-21T15:03:37.803740Z","shell.execute_reply":"2024-02-21T15:03:37.810737Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Вход: \"Работа\"\nПродолженный текст: \"работа не волк . никто не волк . никто не волк . никто не волк . никто не волк . никто не волк . никто не волк . никто не волк .\"\nПричина остановки генерации: \"max tokens\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Как мы видим, модель зациклилась, и начала повторять один и тот же наиболее вероятный кусок текста.\nЭто нередкая проблема для всех генеративных моделей, включая современные LLM.\n\nДля борьбы с ней делают две вещи - нормальный трейн-датасет на этапе трейна, и определенные параметры генерации\nна этапе инференса (например, занизить вероятности в предсказанном распределении у тех токенов, что мы недавно уже генерировали).","metadata":{}},{"cell_type":"markdown","source":"Также можно использовать другие стратегии семплирования, про которые мы поговорим позднее. Например, sample_top_p в реализации класса позволяет семплировать из наиболее вероятных токенов, вероятности которых суммируются в заданное число p. Можете попробовать эту стратегию, правда, возможно ее нужно будет немного подебажить","metadata":{}},{"cell_type":"markdown","source":"### \"Обучим\" LM на n-граммах на полноценном большом датасете","metadata":{}},{"cell_type":"code","source":"%%time\ntokenizer = Tokenizer().build_vocab(all_texts)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:03:46.714916Z","iopub.execute_input":"2024-02-21T15:03:46.715683Z","iopub.status.idle":"2024-02-21T15:03:51.508767Z","shell.execute_reply.started":"2024-02-21T15:03:46.715648Z","shell.execute_reply":"2024-02-21T15:03:51.507812Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"CPU times: user 4.78 s, sys: 7.6 ms, total: 4.79 s\nWall time: 4.79 s\n","output_type":"stream"}]},{"cell_type":"code","source":"len(tokenizer.vocab)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:04:12.067691Z","iopub.execute_input":"2024-02-21T15:04:12.068097Z","iopub.status.idle":"2024-02-21T15:04:12.074714Z","shell.execute_reply.started":"2024-02-21T15:04:12.068065Z","shell.execute_reply":"2024-02-21T15:04:12.073658Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"179367"},"metadata":{}}]},{"cell_type":"markdown","source":"Тут размер словаря посерьезнее","metadata":{}},{"cell_type":"markdown","source":"#### >>>> Задание 2: Оцените, насколько возрастет словарь, если убрать один из этапов предобработки - не приводить все тексты к нижнему регистру\n\nВ качестве ответа используйте отношение размера без предобработка к размеру с предобработкой, округленное как в коде ниже. Например: 15.305","metadata":{}},{"cell_type":"code","source":"preprocessed_size = ...\nwithout_preprocess_size = ...\n\nprint( round(without_preprocess_size / preprocessed_size) )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('tokenizer.pkl', 'wb') as fout:\n    pickle.dump(tokenizer, fout)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:05:36.296909Z","iopub.execute_input":"2024-02-21T15:05:36.297311Z","iopub.status.idle":"2024-02-21T15:05:36.404131Z","shell.execute_reply.started":"2024-02-21T15:05:36.297280Z","shell.execute_reply":"2024-02-21T15:05:36.403088Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train_texts, test_texts = get_dataset(5000, 500)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:12:20.005532Z","iopub.execute_input":"2024-02-21T15:12:20.006258Z","iopub.status.idle":"2024-02-21T15:12:29.164207Z","shell.execute_reply.started":"2024-02-21T15:12:20.006225Z","shell.execute_reply":"2024-02-21T15:12:29.163284Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"(5000, 5)\n(500, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nstat_lm = StatLM(tokenizer, context_size=2, alpha=0.1, sample_top_p = None)\nstat_lm.train(train_texts)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:12:30.367960Z","iopub.execute_input":"2024-02-21T15:12:30.368335Z","iopub.status.idle":"2024-02-21T15:12:42.166853Z","shell.execute_reply.started":"2024-02-21T15:12:30.368305Z","shell.execute_reply":"2024-02-21T15:12:42.165906Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"train lines:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"902d2d11e4a7444fa07b8360fcd66887"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 11.7 s, sys: 150 ms, total: 11.9 s\nWall time: 11.8 s\n","output_type":"stream"}]},{"cell_type":"code","source":"for text in test_texts[:3]:\n    title = text.split('\\n')[0]\n    generated = stat_lm.generate_text(title, max_tokens=128)\n    print(generated['total_text'], '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:12:42.168701Z","iopub.execute_input":"2024-02-21T15:12:42.169089Z","iopub.status.idle":"2024-02-21T15:13:46.500417Z","shell.execute_reply.started":"2024-02-21T15:12:42.169051Z","shell.execute_reply":"2024-02-21T15:13:46.499416Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"в германии объяснили упоминание имени путина на протестах в берлине . в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в \n\nделегации израиля и сша прибыли в оаэ для обсуждения соглашения о сотрудничестве с . в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том \n\nоппозиция белоруссии объявила о создании новой партии вместе с . в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том , что в том \n\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nstat_lm = StatLM(tokenizer, context_size=4, alpha=0.1, sample_top_p = None)\nstat_lm.train(train_texts)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:13:46.501761Z","iopub.execute_input":"2024-02-21T15:13:46.502134Z","iopub.status.idle":"2024-02-21T15:13:59.581842Z","shell.execute_reply.started":"2024-02-21T15:13:46.502100Z","shell.execute_reply":"2024-02-21T15:13:59.580975Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"train lines:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d15fb81797ac4311a389effe17afc6dd"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 12.7 s, sys: 427 ms, total: 13.2 s\nWall time: 13.1 s\n","output_type":"stream"}]},{"cell_type":"code","source":"for text in test_texts[:3]:\n    title = text.split('\\n')[0]\n    generated = stat_lm.generate_text(title, max_tokens=128)\n    print(generated['total_text'], '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:13:59.583999Z","iopub.execute_input":"2024-02-21T15:13:59.584299Z","iopub.status.idle":"2024-02-21T15:15:05.278370Z","shell.execute_reply.started":"2024-02-21T15:13:59.584274Z","shell.execute_reply":"2024-02-21T15:15:05.277382Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"в германии объяснили упоминание имени путина на протестах в берлине рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль \n\nделегации израиля и сша прибыли в оаэ для обсуждения соглашения о сотрудничестве , заключенного 25 ноября 2008 года в разных банках было создано более сотни фиктивных счетов для отмывания денег . чтобы не встретить серьезных препятствий в виде антивирусного обеспечения , мошенники не нападали на государственные организации , а снимали деньги со счетов частных предпринимателей , компаний в небольших городах и даже церквей , сообщается в заявлении компании . дюрхаймер сменит пэфгена и у руля автоспортивного подразделения vw . в свою очередь , в случившемся винит районные центры образования . ректор института зинаида дмитриева сообщила газете . ru в пресс - службе . при этом , по словам главы goldman sachs , банк не мог знать , что случится с американским рынком жилья : подобными спекуляциями занимались все \n\nоппозиция белоруссии объявила о создании новой партии вместе рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль рауль \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Попробуйте перебрать параметры в поисках лучшего качества модели: context_size, alpha, и стратегию генерации sample_top_p=0.9 (возможно, эту стратегию придется немного подебажить)","metadata":{}},{"cell_type":"markdown","source":"## LM на основе RNN","metadata":{}},{"cell_type":"markdown","source":"Рекуррентные сети формально могут обрабатывать последовательность произвольной длины - \nможем просто через RNN-ячейку рекуррентно пропускать последовательность, а в конце посчитать градиент\nпо этой большой цепочке.\n\nНа практике все равно есть ограничения:\n1. Градиент по большой последовательности очень долго считать - с помощью backpropagation through time (BPTT) для каждого элемента последовательности придется считать градиент\n2. Для ускорения обучения и для лучшей сходимости тренировочные примеры (то есть, последовательности = тексты) объединяют во время обучения в батчи. Чтобы батч представить в виде тензора, все последовательности нужно выравнить (мы же не можем в матрице сделать длины строк разными). Для этого используются последовательности с фиксированной длиной, называемой длиной контекста.","metadata":{}},{"cell_type":"markdown","source":"Теперь давайте построим LM с помощью рекуррентной сети","metadata":{}},{"cell_type":"markdown","source":"Попробуем использовать tokenizer из прошлого раздела, уже обученный на словах ","metadata":{}},{"cell_type":"code","source":"with open('tokenizer.pkl', 'rb') as fin:\n    tokenizer = pickle.load(fin)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:15:22.201672Z","iopub.execute_input":"2024-02-21T15:15:22.202036Z","iopub.status.idle":"2024-02-21T15:15:22.317216Z","shell.execute_reply.started":"2024-02-21T15:15:22.202010Z","shell.execute_reply":"2024-02-21T15:15:22.316365Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"len(tokenizer.vocab)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:15:22.989475Z","iopub.execute_input":"2024-02-21T15:15:22.989880Z","iopub.status.idle":"2024-02-21T15:15:22.996299Z","shell.execute_reply.started":"2024-02-21T15:15:22.989850Z","shell.execute_reply":"2024-02-21T15:15:22.995312Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"179367"},"metadata":{}}]},{"cell_type":"markdown","source":"Составим датасеты в torch-формате","metadata":{}},{"cell_type":"code","source":"class NewsDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, inputs: List[List[int]], targets: Optional[List[List[int]]] = None):\n        self.inputs = torch.LongTensor(inputs)\n        self.targets = None\n        if targets is not None:\n            self.targets = torch.LongTensor(targets)\n        \n    def __len__(self):\n        return self.inputs.shape[0]\n\n    def __getitem__(self, idx: int) -> (List[str], int):\n        if self.targets is None:\n            return self.inputs[idx]\n        else:\n            return self.inputs[idx], self.targets[idx]","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:15:25.086448Z","iopub.execute_input":"2024-02-21T15:15:25.086845Z","iopub.status.idle":"2024-02-21T15:15:25.095630Z","shell.execute_reply.started":"2024-02-21T15:15:25.086815Z","shell.execute_reply":"2024-02-21T15:15:25.094722Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"context_size = 32\n\ndef get_tokenized_data(tokenizer: Tokenizer, \n                       texts: List[str], \n                       context_size: int) -> (List[List[int]], List[List[int]]):\n    tokenized_inputs, tokenized_targets = [], []\n    for text in tqdm(texts):\n        tokens = tokenizer.encode(text, append_eos_token=True)\n        for i in range(len(tokens) - context_size):\n            inputs = tokens[i: i + context_size]\n            targets = tokens[i + 1: i + 1 + context_size]\n            tokenized_inputs.append(inputs)\n            tokenized_targets.append(targets)\n    return tokenized_inputs, tokenized_targets","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:15:26.881179Z","iopub.execute_input":"2024-02-21T15:15:26.881701Z","iopub.status.idle":"2024-02-21T15:15:26.888798Z","shell.execute_reply.started":"2024-02-21T15:15:26.881667Z","shell.execute_reply":"2024-02-21T15:15:26.887738Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"Возьмем для начала малое количество текстов, просто чтобы отладить процесс","metadata":{}},{"cell_type":"code","source":"len(train_texts), len(test_texts)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:15:27.708412Z","iopub.execute_input":"2024-02-21T15:15:27.709763Z","iopub.status.idle":"2024-02-21T15:15:27.717477Z","shell.execute_reply.started":"2024-02-21T15:15:27.709716Z","shell.execute_reply":"2024-02-21T15:15:27.716395Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"(5000, 500)"},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 32\n\ntokenized_inputs_train, tokenized_targets_train = get_tokenized_data(tokenizer, train_texts[:120], context_size)\ntokenized_inputs_test, tokenized_targets_test = get_tokenized_data(tokenizer, test_texts[:60], context_size)\n\ntrain_dataset = NewsDataset(tokenized_inputs_train, tokenized_targets_train)\ntest_dataset = NewsDataset(tokenized_inputs_test, tokenized_targets_test)\n\ntrain_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\ntest_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n\nt0, t1 = next(iter(train_dl))\nt0.shape, t1.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:15:29.885722Z","iopub.execute_input":"2024-02-21T15:15:29.886137Z","iopub.status.idle":"2024-02-21T15:15:32.388945Z","shell.execute_reply.started":"2024-02-21T15:15:29.886107Z","shell.execute_reply":"2024-02-21T15:15:32.387996Z"},"trusted":true},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/120 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5f2a19175df4e97ae97b644daf35d93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/60 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69872fd71fad4158b406d0f2f82e6343"}},"metadata":{}},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 32]), torch.Size([32, 32]))"},"metadata":{}}]},{"cell_type":"markdown","source":"Параметры обучения и модели","metadata":{}},{"cell_type":"code","source":"optimizer_params = {}\n\nmodel_params = {\n    'vocab_size': len(tokenizer.vocab),\n    'embed_dim': 300,\n    'hidden_size': 64\n}\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\nn_epochs = 10","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:15:33.474200Z","iopub.execute_input":"2024-02-21T15:15:33.474688Z","iopub.status.idle":"2024-02-21T15:15:33.530087Z","shell.execute_reply.started":"2024-02-21T15:15:33.474657Z","shell.execute_reply":"2024-02-21T15:15:33.528769Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Архитектура сети представлена ниже. \n\nЗдесь мы каждому токену из последовательности сопоставляем обучаемый вектор с помощью слоя nn.Embedding, и всего таких векторов слой знает vocab_size штук, где vocab_size - размер словаря.\n\nКаждый такой обучаемый вектор имеет размерность embed_dim.\n\nПосле того, как мы получим \"умное\" представление всей последовательности - вектор размерности hidden_size, мы хотим отобразить его в пространство словаря, и это отображение (вектор) мы будем использовать как логиты, из которых получается вероятностное распределение для следующего токена. За это отображение отвечает линейный слой.\n\nЗаполните параметры nn.Embedding и nn.Linear с учетом написанного выше","metadata":{}},{"cell_type":"code","source":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\nclass RecLM(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, hidden_size: int):\n        super(RecLM, self).__init__()\n        self.embed = nn.Embedding(..., ...) # TODO заполнить размерности\n        self.rnn = nn.RNNCell(embed_dim, hidden_size, nonlinearity='tanh', bias=True)\n        \n        self.linear = nn.Linear(..., ...)  # TODO заполнить размерности\n        # иногда вместо этого используют ту же матрицу, что в nn.Embedding \n        \n    def forward(self, inputs: torch.LongTensor):\n        \"\"\"\n        inputs: int, bs x seq\n        # pad_mask: bool, bs x seq\n        \"\"\"\n        embed = self.embed(inputs) # bs x seq x dim\n        h_n = None\n        outputs = []\n        for seq_elem in embed.transpose(0, 1):\n            if h_n is None:\n                h_n = self.rnn(seq_elem) # bs x out_dim\n            else:\n                h_n = self.rnn(seq_elem, h_n)\n            outputs.append(h_n)\n        \n        outputs = torch.stack(outputs).transpose(0, 1) # bs x seq x dim\n        return self.linear(outputs) # bs x seq x vocab_size","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:15:35.572397Z","iopub.execute_input":"2024-02-21T15:15:35.573105Z","iopub.status.idle":"2024-02-21T15:15:35.582184Z","shell.execute_reply.started":"2024-02-21T15:15:35.573069Z","shell.execute_reply":"2024-02-21T15:15:35.581210Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"model = RecLM(vocab_size=model_params['vocab_size'], \n              embed_dim=model_params['embed_dim'], \n              hidden_size=model_params['hidden_size'])","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:15:37.179555Z","iopub.execute_input":"2024-02-21T15:15:37.180182Z","iopub.status.idle":"2024-02-21T15:15:37.832692Z","shell.execute_reply.started":"2024-02-21T15:15:37.180153Z","shell.execute_reply":"2024-02-21T15:15:37.831683Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"В качестве лосса используем кросс-энтропию - ведь по сути мы решаем задачу многоклассовой классификации, где классы - это токены из словаря. Почитать про лосс можно ниже:\n\nnn.CrossEntropyLoss: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html","metadata":{}},{"cell_type":"markdown","source":"Напишем класс Trainer , с помощью которого мы будем обучать нейронку. Изучите, что происходит в коде","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    \n    def __init__(\n        self, \n        model_params: dict,\n        optimizer_params: dict, \n        n_epochs: int,\n        train_dataloader: torch.utils.data.DataLoader, \n        val_dataloader: torch.utils.data.DataLoader,\n        device: str\n    ):\n\n        self.model = RecLM(**model_params)\n        self.device = torch.device(device)\n        self.model.to(self.device)\n        self.optimizer = torch.optim.AdamW(params=self.model.parameters(), **optimizer_params)\n        \n        ts = datetime.strftime(datetime.today(), '%Y-%m-%d-%H-%M')\n        LOG_DIR = f'./checkpoints/{ts}'\n        \n        hparams = chain(model_params.items(), optimizer_params.items())\n\n        for k, v in hparams:\n            LOG_DIR += f'-{k}-{v}'\n        \n        self.writer = SummaryWriter(log_dir=LOG_DIR)\n        self.checkpoint_path = LOG_DIR\n\n        self.best_val_loss = float('inf')\n        self.n_epochs = n_epochs\n        self.train_dataloader = train_dataloader\n        self.val_dataloader = val_dataloader\n        \n        self.loss_fn = torch.nn.CrossEntropyLoss(reduction = 'none')\n\n    def iterate_over_dataloader(self, dataloader, suffix: str, epoch: int, update_weights=False):\n        \n        if update_weights:\n            self.model.train()\n        else:\n            self.model.eval()\n        \n        loss_over_epoch = 0\n        num_batches = 0\n\n        for batch in tqdm(dataloader, desc='batches'):\n\n            if update_weights:\n                self.optimizer.zero_grad()\n\n            tokens, labels = batch\n            \n            tokens = tokens.to(self.device)\n            labels = labels.to(self.device)\n            \n            logits = self.model(tokens) # bs x seq x vocab_size\n            \n            loss_values = self.loss_fn(logits.transpose(1, 2), labels) # N x C x seq_len\n            loss_value = loss_values.mean()\n            \n            if update_weights:\n                loss_value.backward()\n                self.optimizer.step()\n            \n            loss_item = loss_value.item()\n            loss_over_epoch += loss_item\n            num_batches += 1\n        \n        avg_loss = loss_over_epoch / num_batches\n        print(f'Epoch {epoch} loss for {suffix}: {avg_loss}')\n        self.writer.add_scalar(tag=f'Loss/{suffix}', scalar_value=avg_loss, global_step=epoch)\n\n        if avg_loss < self.best_val_loss:\n            self.best_val_loss = loss_item\n            torch.save(self.model.state_dict(), os.path.join(self.checkpoint_path, 'rnn_lm.pt'))\n\n            \n    def train_model(self):\n        for epoch_num in tqdm(range(self.n_epochs)):\n            self.iterate_over_dataloader(\n                dataloader=self.train_dataloader, suffix='train', epoch=epoch_num, update_weights=True\n            )\n            self.iterate_over_dataloader(\n                dataloader=self.val_dataloader, suffix='val', epoch=epoch_num\n            )","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:42:30.944240Z","iopub.execute_input":"2024-02-21T15:42:30.945295Z","iopub.status.idle":"2024-02-21T15:42:30.961191Z","shell.execute_reply.started":"2024-02-21T15:42:30.945257Z","shell.execute_reply":"2024-02-21T15:42:30.960169Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"И прежде чем обучать, давайте посмотрим, насколько большая у нас получилась модель.","metadata":{}},{"cell_type":"code","source":"def get_nn_params_stat(model: nn.Module) -> None:\n    \n    def iter_mul(inputs: Iterable) -> int:\n        mul = 1\n        for elem in inputs:\n            mul *= elem\n        return mul\n    \n    shapes = [p.shape for p in model.parameters()]\n    for p_shape in shapes:\n        print(p_shape)\n    total_count = sum([iter_mul(p_shape) for p_shape in shapes])\n    print('Total params:', total_count)\n    print('Model param size in Mb:', total_count * 4 / (2 ** 20))","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:15:39.766500Z","iopub.execute_input":"2024-02-21T15:15:39.766891Z","iopub.status.idle":"2024-02-21T15:15:39.774353Z","shell.execute_reply.started":"2024-02-21T15:15:39.766866Z","shell.execute_reply":"2024-02-21T15:15:39.773432Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"torch.Size([179367, 300])\ntorch.Size([64, 300])\ntorch.Size([64, 64])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([179367, 64])\ntorch.Size([179367])\nTotal params: 65492379\nModel param size in Mb: 249.83359909057617\n","output_type":"stream"}]},{"cell_type":"code","source":"get_nn_params_stat(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:15:43.162310Z","iopub.execute_input":"2024-02-21T15:15:43.163192Z","iopub.status.idle":"2024-02-21T15:15:43.168020Z","shell.execute_reply.started":"2024-02-21T15:15:43.163156Z","shell.execute_reply":"2024-02-21T15:15:43.167092Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"torch.Size([179367, 300])\ntorch.Size([64, 300])\ntorch.Size([64, 64])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([179367, 64])\ntorch.Size([179367])\nTotal params: 65492379\nModel param size in Mb: 249.83359909057617\n","output_type":"stream"}]},{"cell_type":"markdown","source":"В гугл колабе такая модель на гпу не поместится, а если бы и поместилась - обучать такое пришлось бы долго.\n\nКак мы видим, подавляющая часть параметров содержится в тензорах torch.Size([179367, 300]) и torch.Size([179367, 64]) - это параметры слоя эмбеддинга, при этом 179367 - это размерность словаря\n\nИ, как правило, во всех подобных сетях слой nn.Embedding содержит много параметров, и в разных статьях этот слой пытаются как-то \"облегчить\" - либо декомпозировать, либо уменьшить размер словаря. \n\nМы пойдем по пути уменьшения словаря, но для этого нам нужно представить текст не словами, а более оптимальными языковыми единицами. Это достигается с помощью BPE кодирования, которое мы будем обсуджать в курсе. А пока, давайте просто им воспользуемся, после небольшого задания.","metadata":{}},{"cell_type":"code","source":"model = RecLM(vocab_size=model_params['vocab_size'] // 2, \n              embed_dim=model_params['embed_dim'], \n              hidden_size=model_params['hidden_size'])\n\nget_nn_params_stat(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:18:41.782575Z","iopub.execute_input":"2024-02-21T15:18:41.782962Z","iopub.status.idle":"2024-02-21T15:18:42.120143Z","shell.execute_reply.started":"2024-02-21T15:18:41.782932Z","shell.execute_reply":"2024-02-21T15:18:42.118947Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"torch.Size([89683, 300])\ntorch.Size([64, 300])\ntorch.Size([64, 64])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([89683, 64])\ntorch.Size([89683])\nTotal params: 32757719\nModel param size in Mb: 124.96078109741211\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### >>>> Задание 3\n\nпосмотрите, во сколько раз уменьшиться размер модели (в мегабайтах), если уменьшить в два раза размер словаря.  Пример: 5.423","metadata":{}},{"cell_type":"code","source":"round( большее / меньшее, 3 )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### >>>> Задание 4\n\nДопустим, мы знаем, что на нашей гпу-карте мы можем выделить только 15 мб для нашей модели (при этом, все остальные ресурсы под оптимизатор, датасет и прочее уже учтены). В какое минимальное целое кол-во раз нужно уменьшить размер словаря, чтобы выполнить это требование?","metadata":{}},{"cell_type":"code","source":"max_drop = ...\n\nmodel = RecLM(vocab_size=model_params['vocab_size'] // max_drop,\n              embed_dim=model_params['embed_dim'], \n              hidden_size=model_params['hidden_size'])\n\nget_nn_params_stat(model)\nprint(f'Ответ: {max_drop}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ответ - целое число, например 10","metadata":{}},{"cell_type":"markdown","source":"### Уменьшим словарь\n\nОбучим BPE-токенизатор на нашем корпусе. Суть несложная - в качестве токенов мы будем использовать последовательности символов, которые оптимальнее всего сжимают текст (если последовательность токенов встречается очень часто - будем использовать ее как токен, и так итеративно можем построить словарь фиксированного размера). ","metadata":{}},{"cell_type":"code","source":"import tokenizers\nfrom tokenizers import Tokenizer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.processors import TemplateProcessing","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:21:52.213923Z","iopub.execute_input":"2024-02-21T15:21:52.214328Z","iopub.status.idle":"2024-02-21T15:21:52.219645Z","shell.execute_reply.started":"2024-02-21T15:21:52.214298Z","shell.execute_reply":"2024-02-21T15:21:52.218528Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"VOCAB_SIZE = 5_000","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:21:53.600936Z","iopub.execute_input":"2024-02-21T15:21:53.601698Z","iopub.status.idle":"2024-02-21T15:21:53.605988Z","shell.execute_reply.started":"2024-02-21T15:21:53.601666Z","shell.execute_reply":"2024-02-21T15:21:53.604938Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = Whitespace()","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:21:53.817491Z","iopub.execute_input":"2024-02-21T15:21:53.818275Z","iopub.status.idle":"2024-02-21T15:21:53.829598Z","shell.execute_reply.started":"2024-02-21T15:21:53.818245Z","shell.execute_reply":"2024-02-21T15:21:53.828457Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"tokenizer.post_processor = TemplateProcessing(\n    single=\"[BOS] $A [EOS]\",\n    special_tokens=[(\"[BOS]\", 1), (\"[EOS]\", 4)],\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:21:54.208223Z","iopub.execute_input":"2024-02-21T15:21:54.208604Z","iopub.status.idle":"2024-02-21T15:21:54.213383Z","shell.execute_reply.started":"2024-02-21T15:21:54.208573Z","shell.execute_reply":"2024-02-21T15:21:54.212380Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"trainer = BpeTrainer(vocab_size=VOCAB_SIZE, \n                     special_tokens=[\"[UNK]\", \"[BOS]\", \"[SEP]\", \"[PAD]\", \"[EOS]\"],\n                    show_progress=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:21:55.873376Z","iopub.execute_input":"2024-02-21T15:21:55.873748Z","iopub.status.idle":"2024-02-21T15:21:55.878995Z","shell.execute_reply.started":"2024-02-21T15:21:55.873722Z","shell.execute_reply":"2024-02-21T15:21:55.877931Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"def simple_preprocess(text: str) -> str:\n    return text.lower()","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:21:56.221651Z","iopub.execute_input":"2024-02-21T15:21:56.222033Z","iopub.status.idle":"2024-02-21T15:21:56.227796Z","shell.execute_reply.started":"2024-02-21T15:21:56.222003Z","shell.execute_reply":"2024-02-21T15:21:56.226801Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"len(train_texts), len(test_texts)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:21:56.417478Z","iopub.execute_input":"2024-02-21T15:21:56.417784Z","iopub.status.idle":"2024-02-21T15:21:56.424025Z","shell.execute_reply.started":"2024-02-21T15:21:56.417758Z","shell.execute_reply":"2024-02-21T15:21:56.423008Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"(5000, 500)"},"metadata":{}}]},{"cell_type":"code","source":"train_texts = list(map(simple_preprocess, train_texts))\ntest_texts = list(map(simple_preprocess, test_texts))","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:21:58.321141Z","iopub.execute_input":"2024-02-21T15:21:58.321546Z","iopub.status.idle":"2024-02-21T15:21:58.620086Z","shell.execute_reply.started":"2024-02-21T15:21:58.321515Z","shell.execute_reply":"2024-02-21T15:21:58.619119Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"with open('new_train.txt', 'w') as fout:\n    print('\\n'.join(train_texts), file=fout)\n    \nwith open('new_test.txt', 'w') as fout:\n    print('\\n'.join(test_texts), file=fout)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:21:58.840541Z","iopub.execute_input":"2024-02-21T15:21:58.840914Z","iopub.status.idle":"2024-02-21T15:21:59.030530Z","shell.execute_reply.started":"2024-02-21T15:21:58.840885Z","shell.execute_reply":"2024-02-21T15:21:59.029454Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"%%time\nfiles = [f'new_{key}.txt' for key in ['train', 'test']]\ntokenizer.train(files, trainer)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:22:00.157158Z","iopub.execute_input":"2024-02-21T15:22:00.157573Z","iopub.status.idle":"2024-02-21T15:22:05.272325Z","shell.execute_reply.started":"2024-02-21T15:22:00.157540Z","shell.execute_reply":"2024-02-21T15:22:05.271363Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"\n\n\nCPU times: user 14.7 s, sys: 610 ms, total: 15.3 s\nWall time: 5.11 s\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.get_vocab_size()","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:22:05.274092Z","iopub.execute_input":"2024-02-21T15:22:05.274413Z","iopub.status.idle":"2024-02-21T15:22:05.281264Z","shell.execute_reply.started":"2024-02-21T15:22:05.274387Z","shell.execute_reply":"2024-02-21T15:22:05.280263Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"5000"},"metadata":{}}]},{"cell_type":"code","source":"text = 'Вот такие пироги!'\n\nprint(tokenizer.encode(text).tokens)\nprint(tokenizer.encode(text.lower()).tokens)\nprint(tokenizer.encode(text.lower()).ids)\nprint(tokenizer.encode(text.lower()).special_tokens_mask)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:22:05.282261Z","iopub.execute_input":"2024-02-21T15:22:05.282709Z","iopub.status.idle":"2024-02-21T15:22:05.292682Z","shell.execute_reply.started":"2024-02-21T15:22:05.282667Z","shell.execute_reply":"2024-02-21T15:22:05.291735Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"['[BOS]', '[UNK]', 'от', 'такие', 'пи', 'ро', 'ги', '!', '[EOS]']\n['[BOS]', 'вот', 'такие', 'пи', 'ро', 'ги', '!', '[EOS]']\n[1, 1119, 2033, 236, 146, 234, 5, 4]\n[1, 0, 0, 0, 0, 0, 0, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"{ind: token for token, ind in tokenizer.get_vocab().items()}[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:22:35.023692Z","iopub.execute_input":"2024-02-21T15:22:35.024361Z","iopub.status.idle":"2024-02-21T15:22:35.035639Z","shell.execute_reply.started":"2024-02-21T15:22:35.024316Z","shell.execute_reply":"2024-02-21T15:22:35.034667Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"'[UNK]'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.get_vocab()['[EOS]']","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:22:36.072093Z","iopub.execute_input":"2024-02-21T15:22:36.072989Z","iopub.status.idle":"2024-02-21T15:22:36.082186Z","shell.execute_reply.started":"2024-02-21T15:22:36.072952Z","shell.execute_reply":"2024-02-21T15:22:36.081125Z"},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}]},{"cell_type":"markdown","source":"Токенизируем трейн и тест данные","metadata":{}},{"cell_type":"code","source":"context_size = 32\n\ndef get_bpe_tokenized_data(tokenizer: tokenizers.Tokenizer, \n                           texts: List[str], \n                           context_size: int) -> (List[List[int]], List[List[int]]):\n    tokenized_inputs, tokenized_targets = [], []\n    for text in tqdm(texts):\n        tokens = tokenizer.encode(text).ids\n        for i in range(len(tokens) - context_size):\n            inputs = tokens[i: i + context_size]\n            targets = tokens[i + 1: i + 1 + context_size]\n            tokenized_inputs.append(inputs)\n            tokenized_targets.append(targets)\n    return tokenized_inputs, tokenized_targets","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:22:39.354330Z","iopub.execute_input":"2024-02-21T15:22:39.354702Z","iopub.status.idle":"2024-02-21T15:22:39.361746Z","shell.execute_reply.started":"2024-02-21T15:22:39.354675Z","shell.execute_reply":"2024-02-21T15:22:39.360787Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\ntokenized_inputs_train, tokenized_targets_train = get_bpe_tokenized_data(tokenizer, train_texts[:1024], context_size)\ntokenized_inputs_test, tokenized_targets_test = get_bpe_tokenized_data(tokenizer, test_texts[:128], context_size)\n\ntrain_dataset = NewsDataset(tokenized_inputs_train, tokenized_targets_train)\ntest_dataset = NewsDataset(tokenized_inputs_test, tokenized_targets_test)\n\ntrain_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\ntest_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n\nt0, t1 = next(iter(train_dl))\nt0.shape, t1.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:22:52.475234Z","iopub.execute_input":"2024-02-21T15:22:52.476025Z","iopub.status.idle":"2024-02-21T15:23:13.168200Z","shell.execute_reply.started":"2024-02-21T15:22:52.475991Z","shell.execute_reply":"2024-02-21T15:23:13.167254Z"},"trusted":true},"execution_count":96,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1024 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51a612833be5479bba252d9635e85d72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6bbc52eff0a478f97990ca0e450009d"}},"metadata":{}},{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 32]), torch.Size([32, 32]))"},"metadata":{}}]},{"cell_type":"markdown","source":"Чтобы эксперименты можно запустить на Kaggle или Google Colab, выставим небольшие параметры (размер словаря выбран по тем же принципам быстроты экспериментов, но обычно он выбирается 30_000 или в радиусе). \n\nЕсли у вас есть собственные мощности либо свободное время, можете дополнительно поэкспермиентировать с сетями помощнее и перебрать параметры.","metadata":{}},{"cell_type":"code","source":"optimizer_params = {}\n\nmodel_params = {\n    'vocab_size': tokenizer.get_vocab_size(),\n    'embed_dim': 300,\n    'hidden_size': 64\n}\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\nn_epochs = 1","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:23:14.238696Z","iopub.execute_input":"2024-02-21T15:23:14.239621Z","iopub.status.idle":"2024-02-21T15:23:14.246709Z","shell.execute_reply.started":"2024-02-21T15:23:14.239571Z","shell.execute_reply":"2024-02-21T15:23:14.245656Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"model = RecLM(vocab_size=model_params['vocab_size'], \n              embed_dim=model_params['embed_dim'], \n              hidden_size=model_params['hidden_size'])","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:23:18.013601Z","iopub.execute_input":"2024-02-21T15:23:18.014473Z","iopub.status.idle":"2024-02-21T15:23:18.036092Z","shell.execute_reply.started":"2024-02-21T15:23:18.014429Z","shell.execute_reply":"2024-02-21T15:23:18.035366Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"get_nn_params_stat(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:23:18.538920Z","iopub.execute_input":"2024-02-21T15:23:18.539714Z","iopub.status.idle":"2024-02-21T15:23:18.545627Z","shell.execute_reply.started":"2024-02-21T15:23:18.539671Z","shell.execute_reply":"2024-02-21T15:23:18.544165Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"torch.Size([5000, 300])\ntorch.Size([64, 300])\ntorch.Size([64, 64])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([5000, 64])\ntorch.Size([5000])\nTotal params: 1848424\nModel param size in Mb: 7.051177978515625\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Можете оценить, во сколько раз изменился размер сети, при этом весь текст мы кодируем, и никакие токены не теряем (в случае токенизации по словам, выкидывание слова означало потерю сигнала и замену токена на Unknown)\n\nВывод: большую часть параметров подобных сетей содержит слой эмбеддинга. Во многих работах это пытаются исправить: декомпозировать этот слой низкоранговыми матрицами, переиспользовать слой эмбеддингов из начала сети в конце и др. Ну и самое главное - использовать словарь поменьше, 30_000 - классический размер словаря с BPE-токенизацией.","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(model_params, optimizer_params, n_epochs,\n       train_dl, test_dl, device)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:42:48.873043Z","iopub.execute_input":"2024-02-21T15:42:48.873448Z","iopub.status.idle":"2024-02-21T15:42:48.899231Z","shell.execute_reply.started":"2024-02-21T15:42:48.873416Z","shell.execute_reply":"2024-02-21T15:42:48.898334Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:42:53.537602Z","iopub.execute_input":"2024-02-21T15:42:53.537970Z","iopub.status.idle":"2024-02-21T15:42:53.542442Z","shell.execute_reply.started":"2024-02-21T15:42:53.537943Z","shell.execute_reply":"2024-02-21T15:42:53.541410Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"trainer.train_model()","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:42:54.698927Z","iopub.execute_input":"2024-02-21T15:42:54.699303Z","iopub.status.idle":"2024-02-21T15:49:49.150697Z","shell.execute_reply.started":"2024-02-21T15:42:54.699273Z","shell.execute_reply":"2024-02-21T15:49:49.149732Z"},"trusted":true},"execution_count":105,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33495d4a5bfb450096b8672024947084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"batches:   0%|          | 0/34677 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14501f7149be42fc8bf63175c382251a"}},"metadata":{}},{"name":"stdout","text":"Epoch 0 loss for train: 5.032392976688095\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"batches:   0%|          | 0/4603 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b40abb5e5242298e5418cfc6fa6c6e"}},"metadata":{}},{"name":"stdout","text":"Epoch 0 loss for val: 5.512741004076569\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Результат\n\nС помощью обученной модели можно что-нибудь сгенерировать с помощью кода ниже. Поисследуйте поведение модели, насколько разнообразный текс генерируется, насколько текст правдоподбный","metadata":{}},{"cell_type":"markdown","source":"#### >>>> Задание 5.\n\nВ лекциях мы рассматривали, как получить из вектора логитов вероятностное распределение (все числа в векторе суммируются в 1, все числа от 0 до 1). Эта функция также реализована в torch, в модуле torch.nn.functional. Допишите в методе _ get_next_token только эту функцию (при ее добавлении код в ячейках ниже не должен падать), и это же слово используйте в качестве ответа. Например: relu","metadata":{}},{"cell_type":"code","source":"class TextGenerator:\n    def __init__(self, \n                 model: nn.Module, \n                 tokenizer: tokenizers.Tokenizer,\n                 context_size: int,\n                 eos_token_id: int,\n                 sample_top_p: Optional[float] = None):\n        \n        self.model = model.eval()\n        self.tokenizer = tokenizer\n        self.eos_token_id = eos_token_id\n        self.context_size = context_size\n        self.sample_top_p = sample_top_p\n        \n    def sample_token(self, token_distribution: np.ndarray) -> int:\n        if self.sample_top_p is None:\n            return token_distribution.argmax()\n        else:\n            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))))\n            total_proba = 0.0\n            tokens_to_sample = []\n            tokens_probas = []\n            for token_proba, ind in sorted(token_distribution, reverse=True):\n                tokens_to_sample.append(ind)\n                tokens_probas.append(token_proba)\n                total_proba += token_proba\n                if total_proba >= self.sample_top_p:\n                    break\n            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n            tokens_probas = np.array(tokens_probas)\n            tokens_probas = tokens_probas / tokens_probas.sum()\n            return np.random.choice(tokens_to_sample, p=tokens_probas)\n    \n    \n    def _get_next_token(self, tokens: List[int]) -> (int, str):\n        tensor_inputs = torch.LongTensor([tokens])\n        with torch.no_grad():\n            logits = model(tensor_inputs)[0][-1]\n            func = torch.nn.functional. ... # TODO используем функцию из torch.nn.functional, чтобы получить вероятности\n            token_distribution = func(logits)\n            max_proba_ind = self.sample_token(token_distribution.numpy())\n        \n        # print(token_distribution.shape, token_distribution)\n        next_token = self.tokenizer.id_to_token(max_proba_ind)\n        \n        return max_proba_ind, next_token\n            \n    def generate_token(self, text: str, remove_special_tokens: bool = False) -> Dict:\n        tokens = self.tokenizer.encode(text.lower()).ids\n        if tokens[-1] == self.eos_token_id:\n            tokens.pop()\n        tokens = tokens[-self.context_size:]\n        max_proba_ind, next_token = self._get_next_token(tokens)\n        \n        return {\n            'next_token': next_token,\n            'next_token_num': max_proba_ind,\n        }\n    \n    \n    def generate_text(self, \n                      text: str,\n                      max_tokens: int, \n                      remove_special_tokens: bool = False,\n                      ) -> Dict:\n        \n        all_tokens = tokenizer.encode(text.lower()).ids\n        if all_tokens[-1] == self.eos_token_id:\n            all_tokens.pop()\n        tokens = all_tokens[-self.context_size:]\n        if not tokens:\n            return {\n                'all_tokens': all_tokens,\n                'total_text': '',\n                'finish_reason': 'empty_input'\n            }\n        \n        max_proba_ind = None\n        while max_proba_ind != self.eos_token_id and len(all_tokens) < max_tokens:\n            max_proba_ind, next_token = self._get_next_token(tokens)\n            all_tokens.append(max_proba_ind)\n            tokens = all_tokens[-self.context_size:]\n        \n        new_text = self.tokenizer.decode(all_tokens, remove_special_tokens)\n        \n        finish_reason = 'max tokens'\n        if all_tokens[-1] == self.eos_token_id:\n            finish_reason = 'end of text'\n        \n        return {\n            'all_tokens': all_tokens,\n            'total_text': new_text,\n            'finish_reason': finish_reason\n        }","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:49:49.153229Z","iopub.execute_input":"2024-02-21T15:49:49.153702Z","iopub.status.idle":"2024-02-21T15:49:49.174432Z","shell.execute_reply.started":"2024-02-21T15:49:49.153659Z","shell.execute_reply":"2024-02-21T15:49:49.173408Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"text_generator = TextGenerator(model, \n                               tokenizer,\n                               context_size,\n                               tokenizer.token_to_id('[EOS]'), \n                               sample_top_p=None)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:49:49.175784Z","iopub.execute_input":"2024-02-21T15:49:49.176619Z","iopub.status.idle":"2024-02-21T15:49:49.190083Z","shell.execute_reply.started":"2024-02-21T15:49:49.176582Z","shell.execute_reply":"2024-02-21T15:49:49.189241Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"text = \"Вот такие пироги\"\ntext_generator.generate_token(text, )","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:49:49.191778Z","iopub.execute_input":"2024-02-21T15:49:49.192040Z","iopub.status.idle":"2024-02-21T15:49:49.256334Z","shell.execute_reply.started":"2024-02-21T15:49:49.192016Z","shell.execute_reply":"2024-02-21T15:49:49.255298Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1024563228.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  token_distribution = func(logits)\n","output_type":"stream"},{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"{'next_token': 'удовлетво', 'next_token_num': 3513}"},"metadata":{}}]},{"cell_type":"code","source":"text = \"В современном мире \"\ntext_generator.generate_token(text, )","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:49:49.257919Z","iopub.execute_input":"2024-02-21T15:49:49.258588Z","iopub.status.idle":"2024-02-21T15:49:49.267349Z","shell.execute_reply.started":"2024-02-21T15:49:49.258557Z","shell.execute_reply":"2024-02-21T15:49:49.266315Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1024563228.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  token_distribution = func(logits)\n","output_type":"stream"},{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"{'next_token': 'фы', 'next_token_num': 4478}"},"metadata":{}}]},{"cell_type":"code","source":"text = \"В современном мире \"\ntext_generator.generate_text(text, max_tokens=64)['total_text']","metadata":{"execution":{"iopub.status.busy":"2024-02-21T15:49:49.268571Z","iopub.execute_input":"2024-02-21T15:49:49.268868Z","iopub.status.idle":"2024-02-21T15:49:49.385186Z","shell.execute_reply.started":"2024-02-21T15:49:49.268843Z","shell.execute_reply":"2024-02-21T15:49:49.384263Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1024563228.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  token_distribution = func(logits)\n","output_type":"stream"},{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"'[BOS] в совреме нном мире фы результаты а фильм кварти факту именно менять фи стол определе государство v оо стратеги жители кни писа представля вши лась заключи дца добавил цитирует компаний инициати ской важно голки шка колле goo дж 65 паде 75 25 раздо расследование хол результаты а прием премьера страте жители кни писа представля вши лась заключи дца добавил цитирует компаний инициати ской'"},"metadata":{}}]},{"cell_type":"markdown","source":"Модель достаточно простая - малый словарь, мало параметров к RNN, мало обучали, на малом количестве данных. Это приводит к тому, что тексты получаются не самые правдоподобные. Экспериментируя с гиперпараметрами (включая размер выбранного датасета), мне удалось достичь приемлемого качества генерации. Ради интереса, можете попробовать сделать также.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}