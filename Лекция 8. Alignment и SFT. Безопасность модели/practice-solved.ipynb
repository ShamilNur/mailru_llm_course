{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:46:13.04725Z",
     "iopub.status.busy": "2024-04-01T20:46:13.046856Z",
     "iopub.status.idle": "2024-04-01T20:46:13.058846Z",
     "shell.execute_reply": "2024-04-01T20:46:13.057913Z",
     "shell.execute_reply.started": "2024-04-01T20:46:13.04722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.update({\"CUDA_VISIBLE_DEVICES\": \"0\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:46:13.276704Z",
     "iopub.status.busy": "2024-04-01T20:46:13.276133Z",
     "iopub.status.idle": "2024-04-01T20:47:16.041722Z",
     "shell.execute_reply": "2024-04-01T20:47:16.040413Z",
     "shell.execute_reply.started": "2024-04-01T20:46:13.276674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install -q peft\n",
    "! pip install -q jsonlines\n",
    "! pip install -q accelerate\n",
    "! pip install -q -U bitsandbytes\n",
    "! pip install -q trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Воркшоп: RLHF для LLM\n",
    "\n",
    "В этом блоке разбираем обучение с подкреплением на человеческой обратной связи (RLHF). Цель — сделать модели умнее, безопаснее и склонными к рассуждениям. Процесс включает предварительное SFT, обучение модели вознаграждения и финальное RL-обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ключевые алгоритмы RLHF\n",
    "- **PPO** — исторически первый метод, где политика обучается с помощью награды от отдельной reward-модели и KL-штрафа.\n",
    "- **GRPO** — развитие идеи PPO, где учитывается несколько источников награды и более гибкая работа с KL-потерей.\n",
    "- **DPO** — вариант без явной reward-модели. Обучаемся на предпочтениях вида `(промпт, хороший ответ, плохой ответ)` и усиливаем хорошее, ослабляя плохое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Математика PPO\n",
    "Алгоритм оптимизирует суррогатный критерий:\n",
    "$$L_{\text{PPO}}(\theta)=\\mathbb{E}[\\min(r_t(\theta)\\hat A_t,\\ \text{clip}(r_t(\theta),1-\\epsilon,1+\\epsilon)\\hat A_t)]$$\n",
    "где $r_t(\theta)$ — отношение новой и старой политик, $\\hat A_t$ — преимущество. В RLHF роль награды играет модель вознаграждения, а старой политикой служит SFT-модель. Добавление KL штрафа удерживает политку рядом с исходной.\n",
    "Важно проверять, что расчёт преимущества корректен, иначе модель может деградировать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Preference Optimization\n",
    "Из лосса PPO можно вывести формулу оптимизации по предпочтениям:\n",
    "$$L_{\text{DPO}}=-\\log\\sigma(meta(f_\theta(y^+)-f_\theta(y^-)))$$\n",
    "где $f_\theta$ — логиты модели, $y^+$ — предпочтительный ответ, $y^-$ — отклонённый. Старая модель используется при расчёте скрытой награды. Метод избавляет от отдельной reward-модели и стабилизирует обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практические рецепты и ловушки\n",
    "1. Всегда держите под рукой копию SFT-модели — она нужна для стабилизации обучения.\n",
    "2. Следите за качеством данных предпочтений: шум приводит к неустойчивости.\n",
    "3. При использовании PPO контролируйте величину KL-штрафа, иначе модель может забыть исходные знания.\n",
    "4. DPO проще в реализации, но требует аккуратной подготовки пар предпочтений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем модель и токенизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:47:16.044461Z",
     "iopub.status.busy": "2024-04-01T20:47:16.044116Z",
     "iopub.status.idle": "2024-04-01T20:47:23.888139Z",
     "shell.execute_reply": "2024-04-01T20:47:23.887236Z",
     "shell.execute_reply.started": "2024-04-01T20:47:16.044428Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_path = \"openlm-research/open_llama_3b_v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_cache=False,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:47:23.889715Z",
     "iopub.status.busy": "2024-04-01T20:47:23.889283Z",
     "iopub.status.idle": "2024-04-01T20:47:24.012302Z",
     "shell.execute_reply": "2024-04-01T20:47:24.011378Z",
     "shell.execute_reply.started": "2024-04-01T20:47:23.889687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попробуем что-нибудь сгенерировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:47:24.014743Z",
     "iopub.status.busy": "2024-04-01T20:47:24.01445Z",
     "iopub.status.idle": "2024-04-01T20:47:24.032024Z",
     "shell.execute_reply": "2024-04-01T20:47:24.031117Z",
     "shell.execute_reply.started": "2024-04-01T20:47:24.014718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:47:24.03336Z",
     "iopub.status.busy": "2024-04-01T20:47:24.033085Z",
     "iopub.status.idle": "2024-04-01T20:48:04.874066Z",
     "shell.execute_reply": "2024-04-01T20:48:04.87297Z",
     "shell.execute_reply.started": "2024-04-01T20:47:24.033331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "prompt = '### Question: Describe what summer means to you in one sentence.\\n\\n### Answer:'\n",
    "tokens = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "output = model.generate(\n",
    "    inputs=tokens['input_ids'].cuda(),\n",
    "    generation_config=GenerationConfig(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_k=40,\n",
    "        top_p=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0][tokens['input_ids'].shape[-1]:]).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Готовим датасет для обучения и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:48:04.875921Z",
     "iopub.status.busy": "2024-04-01T20:48:04.875336Z",
     "iopub.status.idle": "2024-04-01T20:48:06.021748Z",
     "shell.execute_reply": "2024-04-01T20:48:06.020755Z",
     "shell.execute_reply.started": "2024-04-01T20:48:04.87588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:48:06.02342Z",
     "iopub.status.busy": "2024-04-01T20:48:06.02289Z",
     "iopub.status.idle": "2024-04-01T20:48:06.028914Z",
     "shell.execute_reply": "2024-04-01T20:48:06.027905Z",
     "shell.execute_reply.started": "2024-04-01T20:48:06.023393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process(row):\n",
    "    row[\"prompt\"] = f'### Question: {row[\"prompt\"].strip()}\\n\\n### Answer:'\n",
    "    row[\"chosen\"] = row[\"chosen\"][-1][\"content\"].strip()\n",
    "    row[\"rejected\"] = row[\"rejected\"][-1][\"content\"].strip()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:48:06.030365Z",
     "iopub.status.busy": "2024-04-01T20:48:06.030093Z",
     "iopub.status.idle": "2024-04-01T20:48:18.445977Z",
     "shell.execute_reply": "2024-04-01T20:48:18.44492Z",
     "shell.execute_reply.started": "2024-04-01T20:48:06.030342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:48:18.448228Z",
     "iopub.status.busy": "2024-04-01T20:48:18.447501Z",
     "iopub.status.idle": "2024-04-01T20:48:18.456958Z",
     "shell.execute_reply": "2024-04-01T20:48:18.456004Z",
     "shell.execute_reply.started": "2024-04-01T20:48:18.448181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].select(range(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:48:18.459821Z",
     "iopub.status.busy": "2024-04-01T20:48:18.459538Z",
     "iopub.status.idle": "2024-04-01T20:48:19.631955Z",
     "shell.execute_reply": "2024-04-01T20:48:19.630984Z",
     "shell.execute_reply.started": "2024-04-01T20:48:18.459796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:48:19.633668Z",
     "iopub.status.busy": "2024-04-01T20:48:19.633308Z",
     "iopub.status.idle": "2024-04-01T20:48:19.641143Z",
     "shell.execute_reply": "2024-04-01T20:48:19.639979Z",
     "shell.execute_reply.started": "2024-04-01T20:48:19.633631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    warmup_ratio=0.0,\n",
    "    evaluation_strategy=\"no\",\n",
    "    eval_steps=8,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:48:19.642619Z",
     "iopub.status.busy": "2024-04-01T20:48:19.64235Z",
     "iopub.status.idle": "2024-04-01T20:48:19.997868Z",
     "shell.execute_reply": "2024-04-01T20:48:19.996801Z",
     "shell.execute_reply.started": "2024-04-01T20:48:19.642595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    args=train_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:48:19.999811Z",
     "iopub.status.busy": "2024-04-01T20:48:19.999239Z",
     "iopub.status.idle": "2024-04-01T20:48:21.103347Z",
     "shell.execute_reply": "2024-04-01T20:48:21.102279Z",
     "shell.execute_reply.started": "2024-04-01T20:48:19.999776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:48:21.105401Z",
     "iopub.status.busy": "2024-04-01T20:48:21.105052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
